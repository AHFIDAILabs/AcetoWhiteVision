{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed975bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENHANCED SSL TRAINING WITH AGGRESSIVE OPTIMIZATION\n",
    "# Implements: Focal Loss tuning, threshold optimization,\n",
    "# architecture improvements, training strategies, and ensemble capability\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b483c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\09_AHFID\\CervicalAI-Screen\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Import Required Libraries, Modules, and Packages\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1559ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Setup and Seeds\n",
    "# ============================================================\n",
    "if 'notebook' in os.getcwd():\n",
    "    os.chdir('../')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83aed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. ENHANCED MODEL ARCHITECTURE with Attention\n",
    "# ============================================================\n",
    "class EnhancedSSLEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet_b0', num_classes=2, dropout_rate=0.4):\n",
    "        super(EnhancedSSLEfficientNet, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, self.feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.feature_dim // 4, self.feature_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Deeper classifier with batch normalization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.6),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in [self.attention, self.classifier, self.projector]:\n",
    "            for layer in m:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        attention_weights = self.attention(features)\n",
    "        features = features * attention_weights\n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5ec2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. ENHANCED FOCAL LOSS (Recommendation #2)\n",
    "# ============================================================\n",
    "class AggressiveFocalLoss(nn.Module):\n",
    "    \"\"\"Enhanced focal loss with higher alpha and gamma for sensitivity\"\"\"\n",
    "    def __init__(self, alpha=0.95, gamma=3.0):\n",
    "        super(AggressiveFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([1 - alpha, alpha])\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1)).to(inputs.device)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at * (1 - pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017e52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. AGGRESSIVE DATA AUGMENTATION (Recommendation #2)\n",
    "# ============================================================\n",
    "norm_params = {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}\n",
    "\n",
    "# Very strong augmentation for positives\n",
    "very_strong_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.6),\n",
    "    transforms.RandomVerticalFlip(p=0.6),\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.3, hue=0.15),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.15, 0.15), scale=(0.7, 1.3)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**norm_params)\n",
    "])\n",
    "\n",
    "moderate_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**norm_params)\n",
    "])\n",
    "\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**norm_params)\n",
    "])\n",
    "\n",
    "val_transform = weak_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bf58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Datasets\n",
    "# ============================================================\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform_map):\n",
    "        self.paths, self.labels, self.tm = paths, labels, transform_map\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        return self.tm[label](img), label\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, paths, weak_tf, strong_tf):\n",
    "        self.paths, self.weak_tf, self.strong_tf = paths, weak_tf, strong_tf\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        return self.weak_tf(img), self.strong_tf(img)\n",
    "\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths, self.labels, self.tf = paths, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        return self.tf(img), self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d9fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. Data Loading\n",
    "# ============================================================\n",
    "def load_and_prepare_data():\n",
    "    ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "    with open(ARTIFACTS_DIR / \"data_metadata.json\", \"r\") as f:\n",
    "        data_metadata = json.load(f)\n",
    "    data_dir = Path(data_metadata[\"data_dir\"])\n",
    "    labeled_dir, unlabeled_dir = data_dir / \"labeled\", data_dir / \"unlabeled\"\n",
    "    \n",
    "    labeled_paths, labels = [], []\n",
    "    for name, idx in {\"Negative\": 0, \"Positive\": 1}.items():\n",
    "        class_dir = labeled_dir / name\n",
    "        if class_dir.exists():\n",
    "            for path in class_dir.glob(\"*.[jp][pn]g\"):\n",
    "                labeled_paths.append(str(path))\n",
    "                labels.append(idx)\n",
    "    \n",
    "    unlabeled_paths = [str(p) for p in unlabeled_dir.glob(\"*.[jp][pn]g\")] if unlabeled_dir.exists() else []\n",
    "    print(f\"Loaded {len(labeled_paths)} labeled ({Counter(labels)}) and {len(unlabeled_paths)} unlabeled images.\")\n",
    "    \n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        labeled_paths, labels, test_size=0.2, random_state=SEED, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Use very strong augmentation for positives\n",
    "    train_dataset = LabeledDataset(train_paths, train_labels, \n",
    "                                   transform_map={0: moderate_transform, 1: very_strong_transform})\n",
    "    val_dataset = ValidationDataset(val_paths, val_labels, val_transform)\n",
    "    unlabeled_dataset = UnlabeledDataset(unlabeled_paths, weak_transform, very_strong_transform)\n",
    "    \n",
    "    return train_dataset, val_dataset, unlabeled_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b912d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. ENHANCED TRAINER with Better Threshold Search (Recommendation #3)\n",
    "# ============================================================\n",
    "class EnhancedTrainer:\n",
    "    def __init__(self, model, device='cuda', ssl_weight=0.5, confidence_thresh=0.97):\n",
    "        self.model, self.device = model.to(device), device\n",
    "        self.criterion = AggressiveFocalLoss(alpha=0.95, gamma=3.0)\n",
    "        self.ssl_weight, self.confidence_thresh = ssl_weight, confidence_thresh\n",
    "        \n",
    "        # Lower learning rate with higher weight decay (Recommendation #4)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=1e-5, weight_decay=1e-2\n",
    "        )\n",
    "        \n",
    "        # Cosine annealing with warm restarts (Recommendation #4)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=10, T_mult=2\n",
    "        )\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'consistency_loss': [], \n",
    "            'sensitivity': [], 'specificity': [], \n",
    "            'false_negatives': [], 'false_positives': []\n",
    "        }\n",
    "        self.best_clinical_score = float('inf')\n",
    "        self.best_metrics = {}\n",
    "\n",
    "    def train_epoch(self, labeled_loader, unlabeled_loader, epoch, warmup_epochs):\n",
    "        self.model.train()\n",
    "        total_sup_loss, total_cons_loss = 0, 0\n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "        pbar = tqdm(labeled_loader, desc=f'Epoch {epoch+1}')\n",
    "        \n",
    "        for labeled_images, labels in pbar:\n",
    "            labeled_images, labels = labeled_images.to(self.device), labels.to(self.device)\n",
    "            logits_l = self.model(labeled_images)\n",
    "            sup_loss = self.criterion(logits_l, labels)\n",
    "            total_loss = sup_loss\n",
    "            \n",
    "            if epoch >= warmup_epochs:\n",
    "                try:\n",
    "                    images_u_w, images_u_s = next(unlabeled_iter)\n",
    "                except StopIteration:\n",
    "                    unlabeled_iter = iter(unlabeled_loader)\n",
    "                    images_u_w, images_u_s = next(unlabeled_iter)\n",
    "                images_u_w, images_u_s = images_u_w.to(self.device), images_u_s.to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits_u_w = self.model(images_u_w)\n",
    "                    max_probs, p_targets = torch.max(F.softmax(logits_u_w, dim=1), dim=1)\n",
    "                    mask = max_probs.ge(self.confidence_thresh).float()\n",
    "                \n",
    "                logits_u_s = self.model(images_u_s)\n",
    "                cons_loss = (F.cross_entropy(logits_u_s, p_targets, reduction='none') * mask).mean()\n",
    "                total_loss += self.ssl_weight * cons_loss\n",
    "                total_cons_loss += cons_loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_sup_loss += sup_loss.item()\n",
    "            pbar.set_postfix({'sup_loss': f'{sup_loss.item():.3f}', \n",
    "                            'cons_loss': f'{total_cons_loss / (len(pbar) or 1):.3f}'})\n",
    "        \n",
    "        return total_sup_loss / len(labeled_loader), total_cons_loss / len(labeled_loader)\n",
    "\n",
    "    def validate_and_find_threshold(self, val_loader, fn_target=2, fp_target=3):\n",
    "        \"\"\"Enhanced threshold search prioritizing sensitivity (Recommendation #3)\"\"\"\n",
    "        self.model.eval()\n",
    "        all_probs, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                logits = self.model(images.to(self.device))\n",
    "                all_probs.extend(F.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        all_probs, all_labels = np.array(all_probs), np.array(all_labels)\n",
    "        \n",
    "        # Finer granularity threshold search\n",
    "        best_threshold = -1\n",
    "        best_fp = float('inf')\n",
    "        \n",
    "        # First pass: Find thresholds meeting FN target\n",
    "        fn_acceptable_thresholds = []\n",
    "        for threshold in np.arange(0.01, 0.95, 0.005):  # Finer steps\n",
    "            preds = (all_probs > threshold).astype(int)\n",
    "            fn = np.sum((all_labels == 1) & (preds == 0))\n",
    "            if fn <= fn_target:\n",
    "                fn_acceptable_thresholds.append((threshold, fn))\n",
    "        \n",
    "        # Second pass: Among FN-acceptable, optimize FP\n",
    "        if fn_acceptable_thresholds:\n",
    "            for threshold, fn in fn_acceptable_thresholds:\n",
    "                preds = (all_probs > threshold).astype(int)\n",
    "                fp = np.sum((all_labels == 0) & (preds == 1))\n",
    "                if fp <= fp_target and fp < best_fp:\n",
    "                    best_fp = fp\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        # If no threshold meets both, relax FP constraint\n",
    "        if best_threshold == -1 and fn_acceptable_thresholds:\n",
    "            for threshold, fn in fn_acceptable_thresholds:\n",
    "                preds = (all_probs > threshold).astype(int)\n",
    "                fp = np.sum((all_labels == 0) & (preds == 1))\n",
    "                if fp < best_fp:\n",
    "                    best_fp = fp\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        # Fallback: prioritize FN minimization\n",
    "        if best_threshold == -1:\n",
    "            best_fn = float('inf')\n",
    "            for threshold in np.arange(0.01, 0.95, 0.005):\n",
    "                preds = (all_probs > threshold).astype(int)\n",
    "                fn = np.sum((all_labels == 1) & (preds == 0))\n",
    "                fp = np.sum((all_labels == 0) & (preds == 1))\n",
    "                if fn < best_fn or (fn == best_fn and fp < best_fp):\n",
    "                    best_fn, best_fp, best_threshold = fn, fp, threshold\n",
    "        \n",
    "        y_pred = (all_probs >= best_threshold).astype(int)\n",
    "        tp = np.sum((all_labels == 1) & (y_pred == 1))\n",
    "        fn_final = np.sum((all_labels == 1) & (y_pred == 0))\n",
    "        tn = np.sum((all_labels == 0) & (y_pred == 0))\n",
    "        fp_final = np.sum((all_labels == 0) & (y_pred == 1))\n",
    "        \n",
    "        metrics = {\n",
    "            'sensitivity': tp/(tp+fn_final) if tp+fn_final>0 else 0,\n",
    "            'specificity': tn/(tn+fp_final) if tn+fp_final>0 else 0,\n",
    "            'false_negatives': fn_final,\n",
    "            'false_positives': fp_final\n",
    "        }\n",
    "        return metrics, best_threshold\n",
    "\n",
    "    def train(self, labeled_loader, unlabeled_loader, val_loader, epochs=60, warmup_epochs=5):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED SSL TRAINING WITH AGGRESSIVE OPTIMIZATION\")\n",
    "        print(\"=\"*60)\n",
    "        patience, max_patience = 0, 20  # Increased patience\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, cons_loss = self.train_epoch(labeled_loader, unlabeled_loader, epoch, warmup_epochs)\n",
    "            metrics, threshold = self.validate_and_find_threshold(val_loader, fn_target=2, fp_target=3)\n",
    "            \n",
    "            fn, fp = metrics['false_negatives'], metrics['false_positives']\n",
    "            current_score = (fn * 5) + fp\n",
    "            \n",
    "            # Step scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            for key, value in metrics.items():\n",
    "                self.history[key].append(value)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['consistency_loss'].append(cons_loss)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs} | Sens: {metrics['sensitivity']:.3f}, \"\n",
    "                  f\"Spec: {metrics['specificity']:.3f}, FN: {fn}, FP: {fp}, Thr: {threshold:.3f}\")\n",
    "\n",
    "            is_clinically_acceptable = fn <= 2 and fp <= 3\n",
    "            is_best = False\n",
    "            \n",
    "            if is_clinically_acceptable:\n",
    "                if not self.best_metrics.get('is_clinically_acceptable', False) or current_score < self.best_clinical_score:\n",
    "                    is_best = True\n",
    "            elif not self.best_metrics.get('is_clinically_acceptable', False) and current_score < self.best_clinical_score:\n",
    "                is_best = True\n",
    "\n",
    "            if is_best:\n",
    "                self.best_clinical_score = current_score\n",
    "                self.best_metrics = {**metrics, 'is_clinically_acceptable': is_clinically_acceptable}\n",
    "                patience = 0\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'metrics': metrics,\n",
    "                    'threshold': threshold,\n",
    "                    'history': self.history\n",
    "                }\n",
    "                torch.save(checkpoint, 'artifacts/enhanced_model.pth')\n",
    "                print(f\"  ✓ Saved best model (FN: {fn}, FP: {fp}, Score: {current_score})\")\n",
    "            else:\n",
    "                patience += 1\n",
    "            \n",
    "            if (patience >= max_patience and epoch > 15) or (self.best_metrics.get('is_clinically_acceptable', False) and epoch > 15):\n",
    "                print(f\"\\n{'✓ CLINICAL TARGET MET' if self.best_metrics.get('is_clinically_acceptable', False) else 'Early stopping'}!\")\n",
    "                break\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(f\"Best Results: {self.best_metrics}\")\n",
    "        print(\"=\"*60)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. ENSEMBLE TRAINING\n",
    "# ============================================================\n",
    "def train_ensemble(n_models=3):\n",
    "    \"\"\"Train multiple models with different seeds for ensemble\"\"\"\n",
    "    models = []\n",
    "    trainers = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING ENSEMBLE MODEL {i+1}/{n_models}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Set different seed\n",
    "        seed = SEED + i * 100\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        # Create fresh data loaders\n",
    "        train_dataset, val_dataset, unlabeled_dataset, train_labels = load_and_prepare_data()\n",
    "        \n",
    "        class_counts = np.bincount(train_labels)\n",
    "        class_weights = 1.0 / class_counts\n",
    "        sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        \n",
    "        labeled_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "        unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Create and train model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = EnhancedSSLEfficientNet(num_classes=2, dropout_rate=0.4)\n",
    "        \n",
    "        # Load pretrained weights if available\n",
    "        pretrained_path = Path(\"artifacts/ssl_model_best_010_78.9.pth\")\n",
    "        # if pretrained_path.exists():\n",
    "        #     checkpoint = torch.load(pretrained_path, map_location=device, weights_only=False)\n",
    "        #     model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        if pretrained_path.exists():\n",
    "            checkpoint = torch.load(pretrained_path, map_location=device, weights_only=False)\n",
    "\n",
    "            # Drop mismatched classifier weights\n",
    "            for key in list(checkpoint['model_state_dict'].keys()):\n",
    "                if \"classifier.1.weight\" in key or \"classifier.1.bias\" in key:\n",
    "                    del checkpoint['model_state_dict'][key]\n",
    "\n",
    "            model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "\n",
    "        \n",
    "        trainer = EnhancedTrainer(model, device=device, ssl_weight=0.5, confidence_thresh=0.97)\n",
    "        trainer.train(labeled_loader, unlabeled_loader, val_loader, epochs=60, warmup_epochs=5)\n",
    "        \n",
    "        models.append(model)\n",
    "        trainers.append(trainer)\n",
    "        \n",
    "        # Save individual model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'metrics': trainer.best_metrics\n",
    "        }, f'artifacts/enhanced_model_ensemble_{i}.pth')\n",
    "    \n",
    "    return models, trainers\n",
    "\n",
    "def ensemble_predict(models, val_loader, device):\n",
    "    \"\"\"Make ensemble predictions\"\"\"\n",
    "    all_probs_list = []\n",
    "    all_labels = None\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        probs = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, batch_labels in val_loader:\n",
    "                logits = model(images.to(device))\n",
    "                probs.extend(F.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
    "                labels.extend(batch_labels.numpy())\n",
    "        all_probs_list.append(np.array(probs))\n",
    "        if all_labels is None:\n",
    "            all_labels = np.array(labels)\n",
    "    \n",
    "    # Average probabilities\n",
    "    ensemble_probs = np.mean(all_probs_list, axis=0)\n",
    "    return ensemble_probs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2b53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. Visualization\n",
    "# ============================================================\n",
    "def plot_training_history(history, filename='enhanced_training_history.png'):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Enhanced SSL Training Progress', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes[0, 0].plot(history['train_loss'], label='Supervised Loss')\n",
    "    axes[0, 0].plot(history['consistency_loss'], label='Consistency Loss', linestyle='--')\n",
    "    axes[0, 0].set_title('Training Loss Components')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].plot(history['sensitivity'], label='Sensitivity', color='g')\n",
    "    axes[0, 1].plot(history['specificity'], label='Specificity', color='b')\n",
    "    axes[0, 1].axhline(y=0.90, color='g', linestyle=':', alpha=0.5, label='Target Sens (0.90)')\n",
    "    axes[0, 1].axhline(y=0.85, color='b', linestyle=':', alpha=0.5, label='Target Spec (0.85)')\n",
    "    axes[0, 1].set_title('Sensitivity vs Specificity')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    axes[1, 0].plot(history['false_negatives'], label='False Negatives', color='r', marker='o')\n",
    "    axes[1, 0].axhline(y=2, color='r', linestyle=':', alpha=0.5, label='Target (≤2)')\n",
    "    axes[1, 0].set_title('False Negatives per Epoch')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 1].plot(history['false_positives'], label='False Positives', color='orange', marker='s')\n",
    "    axes[1, 1].axhline(y=3, color='orange', linestyle=':', alpha=0.5, label='Target (≤3)')\n",
    "    axes[1, 1].set_title('False Positives per Epoch')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f'artifacts/{filename}', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41642eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING ENSEMBLE MODEL 1/3\n",
      "============================================================\n",
      "\n",
      "Loaded 190 labeled (Counter({1: 98, 0: 92})) and 6377 unlabeled images.\n",
      "\n",
      "============================================================\n",
      "ENHANCED SSL TRAINING WITH AGGRESSIVE OPTIMIZATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [00:48<00:00,  4.89s/it, sup_loss=0.445, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.415\n",
      "  ✓ Saved best model (FN: 1, FP: 13, Score: 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [00:40<00:00,  4.05s/it, sup_loss=0.189, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/60 | Sens: 0.900, Spec: 0.389, FN: 2, FP: 11, Thr: 0.380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [00:41<00:00,  4.14s/it, sup_loss=0.138, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/60 | Sens: 0.900, Spec: 0.444, FN: 2, FP: 10, Thr: 0.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [00:43<00:00,  4.36s/it, sup_loss=0.095, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/60 | Sens: 0.950, Spec: 0.389, FN: 1, FP: 11, Thr: 0.215\n",
      "  ✓ Saved best model (FN: 1, FP: 11, Score: 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 10/10 [00:45<00:00,  4.54s/it, sup_loss=0.137, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/60 | Sens: 0.900, Spec: 0.389, FN: 2, FP: 11, Thr: 0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 10/10 [02:26<00:00, 14.61s/it, sup_loss=0.445, cons_loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/60 | Sens: 0.900, Spec: 0.111, FN: 2, FP: 16, Thr: 0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 10/10 [02:21<00:00, 14.20s/it, sup_loss=0.443, cons_loss=0.040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/60 | Sens: 0.950, Spec: 0.056, FN: 1, FP: 17, Thr: 0.095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 10/10 [02:18<00:00, 13.89s/it, sup_loss=0.008, cons_loss=0.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/60 | Sens: 0.900, Spec: 0.056, FN: 2, FP: 17, Thr: 0.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [02:20<00:00, 14.08s/it, sup_loss=0.120, cons_loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/60 | Sens: 0.900, Spec: 0.056, FN: 2, FP: 17, Thr: 0.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 10/10 [02:17<00:00, 13.73s/it, sup_loss=0.148, cons_loss=0.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/60 | Sens: 0.950, Spec: 0.111, FN: 1, FP: 16, Thr: 0.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 10/10 [02:17<00:00, 13.72s/it, sup_loss=0.081, cons_loss=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/60 | Sens: 0.900, Spec: 0.056, FN: 2, FP: 17, Thr: 0.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 10/10 [03:58<00:00, 23.89s/it, sup_loss=0.010, cons_loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/60 | Sens: 0.950, Spec: 0.056, FN: 1, FP: 17, Thr: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 10/10 [03:54<00:00, 23.41s/it, sup_loss=0.062, cons_loss=0.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/60 | Sens: 0.900, Spec: 0.167, FN: 2, FP: 15, Thr: 0.070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 10/10 [03:15<00:00, 19.52s/it, sup_loss=0.695, cons_loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/60 | Sens: 1.000, Spec: 0.000, FN: 0, FP: 18, Thr: 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 10/10 [03:40<00:00, 22.03s/it, sup_loss=0.105, cons_loss=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/60 | Sens: 0.950, Spec: 0.056, FN: 1, FP: 17, Thr: 0.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 10/10 [02:53<00:00, 17.39s/it, sup_loss=0.031, cons_loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/60 | Sens: 0.900, Spec: 0.222, FN: 2, FP: 14, Thr: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 10/10 [01:57<00:00, 11.79s/it, sup_loss=0.174, cons_loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/60 | Sens: 0.950, Spec: 0.111, FN: 1, FP: 16, Thr: 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 10/10 [01:55<00:00, 11.51s/it, sup_loss=0.039, cons_loss=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/60 | Sens: 0.950, Spec: 0.000, FN: 1, FP: 18, Thr: 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 10/10 [02:03<00:00, 12.33s/it, sup_loss=0.002, cons_loss=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/60 | Sens: 0.900, Spec: 0.167, FN: 2, FP: 15, Thr: 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 10/10 [02:06<00:00, 12.63s/it, sup_loss=0.135, cons_loss=0.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 10/10 [02:02<00:00, 12.22s/it, sup_loss=0.094, cons_loss=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/60 | Sens: 0.950, Spec: 0.056, FN: 1, FP: 17, Thr: 0.020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 10/10 [01:59<00:00, 11.98s/it, sup_loss=0.043, cons_loss=0.024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/60 | Sens: 0.900, Spec: 0.222, FN: 2, FP: 14, Thr: 0.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 10/10 [02:10<00:00, 13.09s/it, sup_loss=0.056, cons_loss=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/60 | Sens: 0.900, Spec: 0.111, FN: 2, FP: 16, Thr: 0.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 10/10 [02:09<00:00, 12.91s/it, sup_loss=0.030, cons_loss=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/60 | Sens: 0.950, Spec: 0.056, FN: 1, FP: 17, Thr: 0.020\n",
      "\n",
      "Early stopping!\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best Results: {'sensitivity': np.float64(0.95), 'specificity': np.float64(0.3888888888888889), 'false_negatives': np.int64(1), 'false_positives': np.int64(11), 'is_clinically_acceptable': np.False_}\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING ENSEMBLE MODEL 2/3\n",
      "============================================================\n",
      "\n",
      "Loaded 190 labeled (Counter({1: 98, 0: 92})) and 6377 unlabeled images.\n",
      "\n",
      "============================================================\n",
      "ENHANCED SSL TRAINING WITH AGGRESSIVE OPTIMIZATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [00:38<00:00,  3.83s/it, sup_loss=0.190, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/60 | Sens: 1.000, Spec: 0.389, FN: 0, FP: 11, Thr: 0.395\n",
      "  ✓ Saved best model (FN: 0, FP: 11, Score: 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [00:36<00:00,  3.61s/it, sup_loss=0.102, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/60 | Sens: 0.900, Spec: 0.611, FN: 2, FP: 7, Thr: 0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [00:36<00:00,  3.65s/it, sup_loss=0.286, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/60 | Sens: 0.900, Spec: 0.556, FN: 2, FP: 8, Thr: 0.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [00:43<00:00,  4.39s/it, sup_loss=0.184, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/60 | Sens: 1.000, Spec: 0.333, FN: 0, FP: 12, Thr: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 10/10 [00:30<00:00,  3.05s/it, sup_loss=0.177, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/60 | Sens: 1.000, Spec: 0.556, FN: 0, FP: 8, Thr: 0.270\n",
      "  ✓ Saved best model (FN: 0, FP: 8, Score: 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 10/10 [01:46<00:00, 10.61s/it, sup_loss=0.195, cons_loss=0.024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 10/10 [01:46<00:00, 10.70s/it, sup_loss=0.087, cons_loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/60 | Sens: 0.900, Spec: 0.333, FN: 2, FP: 12, Thr: 0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 10/10 [01:43<00:00, 10.36s/it, sup_loss=0.473, cons_loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/60 | Sens: 0.900, Spec: 0.333, FN: 2, FP: 12, Thr: 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [01:48<00:00, 10.87s/it, sup_loss=0.087, cons_loss=0.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 10/10 [02:02<00:00, 12.29s/it, sup_loss=0.178, cons_loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/60 | Sens: 0.950, Spec: 0.333, FN: 1, FP: 12, Thr: 0.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 10/10 [02:07<00:00, 12.70s/it, sup_loss=0.039, cons_loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/60 | Sens: 0.900, Spec: 0.333, FN: 2, FP: 12, Thr: 0.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 10/10 [02:33<00:00, 15.39s/it, sup_loss=0.039, cons_loss=0.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 10/10 [01:57<00:00, 11.77s/it, sup_loss=0.236, cons_loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/60 | Sens: 0.900, Spec: 0.333, FN: 2, FP: 12, Thr: 0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 10/10 [01:59<00:00, 11.98s/it, sup_loss=0.018, cons_loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 10/10 [01:55<00:00, 11.53s/it, sup_loss=0.039, cons_loss=0.010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/60 | Sens: 0.950, Spec: 0.222, FN: 1, FP: 14, Thr: 0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 10/10 [01:59<00:00, 11.99s/it, sup_loss=0.276, cons_loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 10/10 [01:55<00:00, 11.53s/it, sup_loss=0.255, cons_loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/60 | Sens: 0.950, Spec: 0.333, FN: 1, FP: 12, Thr: 0.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 10/10 [01:56<00:00, 11.68s/it, sup_loss=0.279, cons_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 10/10 [01:56<00:00, 11.64s/it, sup_loss=0.635, cons_loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/60 | Sens: 0.900, Spec: 0.222, FN: 2, FP: 14, Thr: 0.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 10/10 [01:55<00:00, 11.55s/it, sup_loss=0.465, cons_loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 10/10 [02:04<00:00, 12.43s/it, sup_loss=0.052, cons_loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/60 | Sens: 0.900, Spec: 0.500, FN: 2, FP: 9, Thr: 0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 10/10 [01:55<00:00, 11.56s/it, sup_loss=0.146, cons_loss=0.023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/60 | Sens: 0.950, Spec: 0.444, FN: 1, FP: 10, Thr: 0.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 10/10 [01:57<00:00, 11.78s/it, sup_loss=0.128, cons_loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/60 | Sens: 0.900, Spec: 0.389, FN: 2, FP: 11, Thr: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 10/10 [01:53<00:00, 11.37s/it, sup_loss=0.012, cons_loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/60 | Sens: 0.900, Spec: 0.500, FN: 2, FP: 9, Thr: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 10/10 [01:55<00:00, 11.54s/it, sup_loss=0.090, cons_loss=0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/60 | Sens: 0.950, Spec: 0.389, FN: 1, FP: 11, Thr: 0.110\n",
      "\n",
      "Early stopping!\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best Results: {'sensitivity': np.float64(1.0), 'specificity': np.float64(0.5555555555555556), 'false_negatives': np.int64(0), 'false_positives': np.int64(8), 'is_clinically_acceptable': np.False_}\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING ENSEMBLE MODEL 3/3\n",
      "============================================================\n",
      "\n",
      "Loaded 190 labeled (Counter({1: 98, 0: 92})) and 6377 unlabeled images.\n",
      "\n",
      "============================================================\n",
      "ENHANCED SSL TRAINING WITH AGGRESSIVE OPTIMIZATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [00:34<00:00,  3.47s/it, sup_loss=0.054, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/60 | Sens: 0.900, Spec: 0.111, FN: 2, FP: 16, Thr: 0.420\n",
      "  ✓ Saved best model (FN: 2, FP: 16, Score: 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [00:37<00:00,  3.76s/it, sup_loss=0.180, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/60 | Sens: 1.000, Spec: 0.000, FN: 0, FP: 18, Thr: 0.010\n",
      "  ✓ Saved best model (FN: 0, FP: 18, Score: 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [00:36<00:00,  3.69s/it, sup_loss=0.080, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [00:35<00:00,  3.56s/it, sup_loss=0.027, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/60 | Sens: 0.950, Spec: 0.056, FN: 1, FP: 17, Thr: 0.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 10/10 [00:33<00:00,  3.35s/it, sup_loss=0.051, cons_loss=0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/60 | Sens: 0.900, Spec: 0.167, FN: 2, FP: 15, Thr: 0.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 10/10 [01:55<00:00, 11.52s/it, sup_loss=0.023, cons_loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 10/10 [02:00<00:00, 12.02s/it, sup_loss=0.063, cons_loss=0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/60 | Sens: 0.950, Spec: 0.222, FN: 1, FP: 14, Thr: 0.180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 10/10 [01:55<00:00, 11.59s/it, sup_loss=0.008, cons_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/60 | Sens: 0.950, Spec: 0.222, FN: 1, FP: 14, Thr: 0.165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [01:53<00:00, 11.40s/it, sup_loss=0.252, cons_loss=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 10/10 [01:55<00:00, 11.52s/it, sup_loss=0.026, cons_loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 10/10 [01:54<00:00, 11.41s/it, sup_loss=0.181, cons_loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 10/10 [01:56<00:00, 11.66s/it, sup_loss=0.053, cons_loss=0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 10/10 [01:54<00:00, 11.41s/it, sup_loss=0.003, cons_loss=0.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/60 | Sens: 0.900, Spec: 0.333, FN: 2, FP: 12, Thr: 0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 10/10 [02:08<00:00, 12.85s/it, sup_loss=0.040, cons_loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 10/10 [02:12<00:00, 13.23s/it, sup_loss=0.203, cons_loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 10/10 [02:21<00:00, 14.11s/it, sup_loss=0.017, cons_loss=0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 10/10 [02:28<00:00, 14.84s/it, sup_loss=0.043, cons_loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/60 | Sens: 0.900, Spec: 0.333, FN: 2, FP: 12, Thr: 0.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 10/10 [02:09<00:00, 12.95s/it, sup_loss=0.020, cons_loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 10/10 [01:58<00:00, 11.81s/it, sup_loss=0.085, cons_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 10/10 [01:57<00:00, 11.78s/it, sup_loss=0.082, cons_loss=0.025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/60 | Sens: 0.900, Spec: 0.389, FN: 2, FP: 11, Thr: 0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 10/10 [01:56<00:00, 11.67s/it, sup_loss=0.068, cons_loss=0.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/60 | Sens: 0.900, Spec: 0.278, FN: 2, FP: 13, Thr: 0.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 10/10 [01:54<00:00, 11.43s/it, sup_loss=0.013, cons_loss=0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/60 | Sens: 0.950, Spec: 0.278, FN: 1, FP: 13, Thr: 0.145\n",
      "\n",
      "Early stopping!\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "Best Results: {'sensitivity': np.float64(1.0), 'specificity': np.float64(0.0), 'false_negatives': np.int64(0), 'false_positives': np.int64(18), 'is_clinically_acceptable': np.False_}\n",
      "============================================================\n",
      "Loaded 190 labeled (Counter({1: 98, 0: 92})) and 6377 unlabeled images.\n",
      "\n",
      "============================================================\n",
      "ENSEMBLE RESULTS\n",
      "============================================================\n",
      "Sensitivity: 1.000\n",
      "Specificity: 0.000\n",
      "False Negatives: 0\n",
      "False Positives: 18\n",
      "Threshold: 0.010\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. Main Execution\n",
    "# ============================================================\n",
    "def convert_numpy_to_python(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy_to_python(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [convert_numpy_to_python(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "def main_single_model():\n",
    "    \"\"\"Train single enhanced model\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENHANCED SSL TRAINING - SINGLE MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    train_dataset, val_dataset, unlabeled_dataset, train_labels = load_and_prepare_data()\n",
    "    \n",
    "    class_counts = np.bincount(train_labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    labeled_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    print(f\"\\nData loader summary: Labeled Batches: {len(labeled_loader)}, Unlabeled Batches: {len(unlabeled_loader)}\")\n",
    "    \n",
    "    model = EnhancedSSLEfficientNet(num_classes=2, dropout_rate=0.4)\n",
    "    print(f\"\\nModel summary: Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    pretrained_path = ARTIFACTS_DIR / \"ssl_model_best_010_78.9.pth\"\n",
    "    if pretrained_path.exists():\n",
    "        print(f\"Loading pretrained weights from: {pretrained_path}\")\n",
    "        checkpoint = torch.load(pretrained_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        print(\"✓ Pretrained weights loaded\")\n",
    "    \n",
    "    trainer = EnhancedTrainer(model, device=device, ssl_weight=0.5, confidence_thresh=0.97)\n",
    "    history = trainer.train(labeled_loader, unlabeled_loader, val_loader, epochs=60, warmup_epochs=5)\n",
    "    \n",
    "    plot_training_history(history)\n",
    "    \n",
    "    history_serializable = convert_numpy_to_python(history)\n",
    "    with open(ARTIFACTS_DIR / \"enhanced_training_history.json\", \"w\") as f:\n",
    "        json.dump(history_serializable, f, indent=2)\n",
    "    \n",
    "    return trainer, history\n",
    "\n",
    "def main_ensemble():\n",
    "    \"\"\"Train ensemble of models\"\"\"\n",
    "    models, trainers = train_ensemble(n_models=3)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    _, val_dataset, _, _ = load_and_prepare_data()\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ensemble_probs, all_labels = ensemble_predict(models, val_loader, device)\n",
    "    \n",
    "    # Find best threshold for ensemble\n",
    "    best_threshold = -1\n",
    "    best_fn = float('inf')\n",
    "    best_fp = float('inf')\n",
    "    \n",
    "    for threshold in np.arange(0.01, 0.95, 0.005):\n",
    "        preds = (ensemble_probs > threshold).astype(int)\n",
    "        fn = np.sum((all_labels == 1) & (preds == 0))\n",
    "        fp = np.sum((all_labels == 0) & (preds == 1))\n",
    "        if fn <= 2 and fp <= 3:\n",
    "            best_threshold = threshold\n",
    "            best_fn, best_fp = fn, fp\n",
    "            break\n",
    "        if fn < best_fn or (fn == best_fn and fp < best_fp):\n",
    "            best_fn, best_fp, best_threshold = fn, fp, threshold\n",
    "    \n",
    "    y_pred = (ensemble_probs >= best_threshold).astype(int)\n",
    "    tp = np.sum((all_labels == 1) & (y_pred == 1))\n",
    "    tn = np.sum((all_labels == 0) & (y_pred == 0))\n",
    "    \n",
    "    ensemble_metrics = {\n",
    "        'sensitivity': tp/(tp+best_fn) if tp+best_fn>0 else 0,\n",
    "        'specificity': tn/(tn+best_fp) if tn+best_fp>0 else 0,\n",
    "        'false_negatives': best_fn,\n",
    "        'false_positives': best_fp,\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENSEMBLE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Sensitivity: {ensemble_metrics['sensitivity']:.3f}\")\n",
    "    print(f\"Specificity: {ensemble_metrics['specificity']:.3f}\")\n",
    "    print(f\"False Negatives: {ensemble_metrics['false_negatives']}\")\n",
    "    print(f\"False Positives: {ensemble_metrics['false_positives']}\")\n",
    "    print(f\"Threshold: {ensemble_metrics['threshold']:.3f}\")\n",
    "    \n",
    "    return models, trainers, ensemble_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose mode\n",
    "    USE_ENSEMBLE = True  # Set to False for single model training\n",
    "    \n",
    "    if USE_ENSEMBLE:\n",
    "        models, trainers, ensemble_metrics = main_ensemble()\n",
    "    else:\n",
    "        trainer, history = main_single_model()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if trainer.best_metrics:\n",
    "            best_metrics = trainer.best_metrics\n",
    "            print(f\"Best Model Performance:\")\n",
    "            print(f\"  - False Negatives: {best_metrics['false_negatives']}\")\n",
    "            print(f\"  - False Positives: {best_metrics['false_positives']}\")\n",
    "            print(f\"  - Sensitivity: {best_metrics['sensitivity']:.3f}\")\n",
    "            print(f\"  - Specificity: {best_metrics['specificity']:.3f}\")\n",
    "            print(f\"  - Threshold: {best_metrics['threshold']:.3f}\")\n",
    "            print(f\"  - Best Epoch: {best_metrics['epoch']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
