{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9e0f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\09_AHFID\\\\CervicalAI-Screen\\\\notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7435ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\09_AHFID\\\\CervicalAI-Screen'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b19eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_data_ingestion.ipynb\n",
    "# Download and organize cervical image data for semi-supervised learning\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import json\n",
    "import urllib.request\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5923f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\09_AHFID\\CervicalAI-Screen\n",
      "Artifacts directory: c:\\09_AHFID\\CervicalAI-Screen\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "RAW_DATA_DIR = ARTIFACTS_DIR / \"raw_data\"\n",
    "PROCESSED_DATA_DIR = ARTIFACTS_DIR / \"via_cervix_ssl\"\n",
    "\n",
    "# Create directories\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "RAW_DATA_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Artifacts directory: {ARTIFACTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8454395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gdrive(file_id, output_path):\n",
    "    \"\"\"Download file from Google Drive using gdown\"\"\"\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading data from Google Drive...\")\n",
    "        print(f\"URL: {url}\")\n",
    "        gdown.download(url, str(output_path), quiet=False)\n",
    "        print(f\"Download completed: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"1. File ID is correct\")\n",
    "        print(\"2. File sharing is enabled (Anyone with the link can view)\")\n",
    "        print(\"3. Internet connection is stable\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6119f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_url(url, output_path):\n",
    "    \"\"\"Download file from direct URL\"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading from URL: {url}\")\n",
    "        urllib.request.urlretrieve(url, output_path)\n",
    "        print(f\"Download completed: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c34af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_archive(archive_path, extract_to):\n",
    "    \"\"\"Extract zip or tar archive\"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting {archive_path} to {extract_to}\")\n",
    "        \n",
    "        if archive_path.suffix.lower() == '.zip':\n",
    "            with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "        elif archive_path.suffix.lower() in ['.tar', '.tar.gz', '.tgz']:\n",
    "            with tarfile.open(archive_path, 'r:*') as tar_ref:\n",
    "                tar_ref.extractall(extract_to)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n",
    "            \n",
    "        print(\"Extraction completed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction failed: {e}\")\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24eb6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(source_dir, target_dir, class_name):\n",
    "    \"\"\"Copy images from source to target directory\"\"\"\n",
    "    count = 0\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
    "    \n",
    "    if not source_dir.exists():\n",
    "        print(f\"Warning: {source_dir} does not exist\")\n",
    "        return 0\n",
    "    \n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for img_path in source_dir.rglob(\"*\"):\n",
    "        if img_path.is_file() and img_path.suffix.lower() in valid_extensions:\n",
    "            target_path = target_dir / img_path.name\n",
    "            \n",
    "            # Handle duplicate names\n",
    "            counter = 1\n",
    "            original_target = target_path\n",
    "            while target_path.exists():\n",
    "                stem = original_target.stem\n",
    "                suffix = original_target.suffix\n",
    "                target_path = target_dir / f\"{stem}_{counter}{suffix}\"\n",
    "                counter += 1\n",
    "            \n",
    "            try:\n",
    "                shutil.copy2(img_path, target_path)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {img_path}: {e}\")\n",
    "    \n",
    "    print(f\"Copied {count} images from {class_name}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06fdf52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_directory(extract_dir):\n",
    "    \"\"\"Find the actual data directory within extracted files\"\"\"\n",
    "    possible_dirs = []\n",
    "    \n",
    "    # Look for common data directory patterns\n",
    "    for item in extract_dir.rglob(\"*\"):\n",
    "        if item.is_dir():\n",
    "            dir_name = item.name.lower()\n",
    "            # Common patterns for medical image datasets\n",
    "            if any(pattern in dir_name for pattern in ['cervix', 'cervical', 'via', 'data', 'images']):\n",
    "                possible_dirs.append(item)\n",
    "    \n",
    "    # Check for directories containing class folders\n",
    "    for item in extract_dir.rglob(\"*\"):\n",
    "        if item.is_dir():\n",
    "            subdirs = [d.name for d in item.iterdir() if d.is_dir()]\n",
    "            # Look for medical classification patterns\n",
    "            if any(cls in subdirs for cls in ['Negative', 'Positive', 'Suspicious', 'Normal', 'Abnormal']):\n",
    "                possible_dirs.append(item)\n",
    "    \n",
    "    if not possible_dirs:\n",
    "        # Default to extraction directory itself\n",
    "        possible_dirs = [extract_dir]\n",
    "    \n",
    "    # Return the most likely candidate (prefer deeper nested directories)\n",
    "    return max(possible_dirs, key=lambda x: len(x.parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef26ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_ssl_structure(source_dir, target_dir):\n",
    "    \"\"\"Organize data into SSL structure with binary classification\"\"\"\n",
    "    \n",
    "    # Create SSL directory structure\n",
    "    labeled_dir = target_dir / \"labeled\"\n",
    "    unlabeled_dir = target_dir / \"unlabeled\"\n",
    "    \n",
    "    labeled_dir.mkdir(parents=True, exist_ok=True)\n",
    "    unlabeled_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Binary classification: Negative vs Positive (includes suspicious)\n",
    "    negative_dir = labeled_dir / \"Negative\"\n",
    "    positive_dir = labeled_dir / \"Positive\"\n",
    "    \n",
    "    negative_dir.mkdir(parents=True, exist_ok=True)\n",
    "    positive_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find actual data directory\n",
    "    actual_source = find_data_directory(source_dir)\n",
    "    print(f\"Using source directory: {actual_source}\")\n",
    "    \n",
    "    # Map source folders to target classes\n",
    "    folder_mapping = {\n",
    "        # Original folder name -> target class\n",
    "        'Negative': 'Negative',\n",
    "        'Normal': 'Negative',\n",
    "        'negative': 'Negative',\n",
    "        'normal': 'Negative',\n",
    "        'Positive': 'Positive', \n",
    "        'positive': 'Positive',\n",
    "        'Suspicious cancer': 'Positive',\n",
    "        'Suspicious': 'Positive',\n",
    "        'suspicious': 'Positive',\n",
    "        'Cancer': 'Positive',\n",
    "        'cancer': 'Positive',\n",
    "        'Abnormal': 'Positive',\n",
    "        'abnormal': 'Positive'\n",
    "    }\n",
    "    \n",
    "    total_labeled = 0\n",
    "    class_counts = Counter()\n",
    "    \n",
    "    # Process labeled data\n",
    "    for item in actual_source.iterdir():\n",
    "        if item.is_dir():\n",
    "            folder_name = item.name\n",
    "            target_class = folder_mapping.get(folder_name)\n",
    "            \n",
    "            if target_class == 'Negative':\n",
    "                count = copy_images(item, negative_dir, folder_name)\n",
    "                class_counts['Negative'] += count\n",
    "                total_labeled += count\n",
    "            elif target_class == 'Positive':\n",
    "                count = copy_images(item, positive_dir, folder_name)\n",
    "                class_counts['Positive'] += count\n",
    "                total_labeled += count\n",
    "            elif folder_name.lower() in ['unlabeled', 'unlabelled', 'unknown']:\n",
    "                # Handle unlabeled data\n",
    "                count = copy_images(item, unlabeled_dir, folder_name)\n",
    "                class_counts['Unlabeled'] += count\n",
    "            else:\n",
    "                print(f\"Unknown folder: {folder_name} - skipping\")\n",
    "    \n",
    "    # Handle case where all images are in a single directory\n",
    "    if total_labeled == 0:\n",
    "        print(\"No class folders found. Checking for direct image files...\")\n",
    "        image_files = [f for f in actual_source.iterdir() \n",
    "                      if f.is_file() and f.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}]\n",
    "        \n",
    "        if image_files:\n",
    "            print(f\"Found {len(image_files)} images without labels - treating as unlabeled\")\n",
    "            for img_file in image_files:\n",
    "                try:\n",
    "                    shutil.copy2(img_file, unlabeled_dir / img_file.name)\n",
    "                    class_counts['Unlabeled'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {img_file}: {e}\")\n",
    "    \n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b98a8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_summary(data_dir, class_counts):\n",
    "    \"\"\"Print comprehensive data summary\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA ORGANIZATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    labeled_dir = data_dir / \"labeled\"\n",
    "    unlabeled_dir = data_dir / \"unlabeled\"\n",
    "    \n",
    "    # Count actual files\n",
    "    print(\"LABELED DATA:\")\n",
    "    actual_labeled = 0\n",
    "    for class_dir in labeled_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            count = len([f for f in class_dir.iterdir() if f.is_file()])\n",
    "            print(f\"  {class_dir.name}: {count} images\")\n",
    "            actual_labeled += count\n",
    "    \n",
    "    print(f\"  Total labeled: {actual_labeled} images\")\n",
    "    \n",
    "    # Count unlabeled data  \n",
    "    print(\"\\nUNLABELED DATA:\")\n",
    "    actual_unlabeled = 0\n",
    "    if unlabeled_dir.exists():\n",
    "        actual_unlabeled = len([f for f in unlabeled_dir.iterdir() if f.is_file()])\n",
    "        print(f\"  Unlabeled: {actual_unlabeled} images\")\n",
    "    else:\n",
    "        print(\"  No unlabeled data found\")\n",
    "    \n",
    "    total_images = actual_labeled + actual_unlabeled\n",
    "    print(f\"\\nTOTAL DATASET: {total_images} images\")\n",
    "    \n",
    "    if actual_labeled > 0 and actual_unlabeled > 0:\n",
    "        ratio = actual_unlabeled / actual_labeled\n",
    "        print(f\"Unlabeled/Labeled ratio: {ratio:.1f}:1\")\n",
    "        print(f\"SSL data ratio: {ratio:.1f}x more unlabeled data\")\n",
    "        \n",
    "        # SSL suitability assessment\n",
    "        if ratio >= 5:\n",
    "            print(\"✓ Excellent ratio for semi-supervised learning\")\n",
    "        elif ratio >= 3:\n",
    "            print(\"✓ Very good ratio for semi-supervised learning\") \n",
    "        elif ratio >= 1:\n",
    "            print(\"✓ Good ratio for semi-supervised learning\")\n",
    "        else:\n",
    "            print(\"⚠ Limited unlabeled data - SSL benefits may be modest\")\n",
    "    elif actual_unlabeled == 0:\n",
    "        print(\"⚠ No unlabeled data found - will use supervised learning only\")\n",
    "    \n",
    "    # Binary classification assessment\n",
    "    print(f\"\\nBINARY CLASSIFICATION SETUP:\")\n",
    "    print(f\"  Class 0 (Negative): Normal/healthy cervical images\")\n",
    "    print(f\"  Class 1 (Positive): Abnormal/suspicious cervical images\")\n",
    "    print(f\"  This binary setup is optimal for screening applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afa1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ssl_structure(data_dir):\n",
    "    \"\"\"Validate the SSL data structure\"\"\"\n",
    "    print(\"\\nVALIDATING SSL DATA STRUCTURE...\")\n",
    "    \n",
    "    labeled_dir = data_dir / \"labeled\"\n",
    "    unlabeled_dir = data_dir / \"unlabeled\"\n",
    "    \n",
    "    issues = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check labeled data structure\n",
    "    if not labeled_dir.exists():\n",
    "        issues.append(\"Missing labeled directory\")\n",
    "    else:\n",
    "        required_classes = [\"Negative\", \"Positive\"]\n",
    "        for cls in required_classes:\n",
    "            cls_dir = labeled_dir / cls\n",
    "            if not cls_dir.exists():\n",
    "                issues.append(f\"Missing class directory: {cls}\")\n",
    "            else:\n",
    "                count = len([f for f in cls_dir.iterdir() if f.is_file()])\n",
    "                if count == 0:\n",
    "                    issues.append(f\"Empty class directory: {cls}\")\n",
    "                elif count < 10:\n",
    "                    warnings.append(f\"Very few samples in {cls}: {count} images\")\n",
    "    \n",
    "    # Check unlabeled data\n",
    "    if not unlabeled_dir.exists():\n",
    "        warnings.append(\"Missing unlabeled directory - SSL benefits limited\")\n",
    "    else:\n",
    "        count = len([f for f in unlabeled_dir.iterdir() if f.is_file()])\n",
    "        if count == 0:\n",
    "            warnings.append(\"No unlabeled images found - SSL benefits limited\")\n",
    "        elif count < 50:\n",
    "            warnings.append(f\"Few unlabeled samples: {count} images\")\n",
    "    \n",
    "    # Print results\n",
    "    if issues:\n",
    "        print(\"❌ CRITICAL ISSUES FOUND:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✅ Data structure validation passed\")\n",
    "        \n",
    "    if warnings:\n",
    "        print(\"⚠ WARNINGS:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"  - {warning}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f708d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_manifest(data_dir):\n",
    "    \"\"\"Create a detailed manifest of the organized data\"\"\"\n",
    "    manifest = {\n",
    "        \"created_at\": str(Path.cwd()),\n",
    "        \"data_structure\": \"semi_supervised_learning\",\n",
    "        \"classification_type\": \"binary\",\n",
    "        \"classes\": {\n",
    "            \"0\": \"Negative (Normal/Healthy)\",\n",
    "            \"1\": \"Positive (Abnormal/Suspicious)\"\n",
    "        },\n",
    "        \"directories\": {\n",
    "            \"labeled\": str(data_dir / \"labeled\"),\n",
    "            \"unlabeled\": str(data_dir / \"unlabeled\")\n",
    "        },\n",
    "        \"file_counts\": {}\n",
    "    }\n",
    "    \n",
    "    # Count files in each directory\n",
    "    labeled_dir = data_dir / \"labeled\"\n",
    "    unlabeled_dir = data_dir / \"unlabeled\"\n",
    "    \n",
    "    for class_dir in labeled_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            count = len([f for f in class_dir.iterdir() if f.is_file()])\n",
    "            manifest[\"file_counts\"][class_dir.name] = count\n",
    "    \n",
    "    if unlabeled_dir.exists():\n",
    "        count = len([f for f in unlabeled_dir.iterdir() if f.is_file()])\n",
    "        manifest[\"file_counts\"][\"Unlabeled\"] = count\n",
    "    \n",
    "    # Save manifest\n",
    "    manifest_path = data_dir / \"data_manifest.json\"\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"Data manifest saved: {manifest_path}\")\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd252cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"CERVICAL IMAGE DATA INGESTION FOR SSL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Configuration options\n",
    "    print(\"Data source options:\")\n",
    "    print(\"1. Google Drive file ID\")\n",
    "    print(\"2. Direct URL\")\n",
    "    print(\"3. Local file path\")\n",
    "    print(\"4. Skip download (data already extracted)\")\n",
    "    \n",
    "    # For this script, you need to configure your data source:\n",
    "    \n",
    "    # Option 1: Google Drive\n",
    "    GDRIVE_FILE_ID = \"1Sw8aSal9R2Kh7PoD6Ma3QpXevCh2Kcrs\" \n",
    "    \n",
    "    # Option 2: Direct URL\n",
    "    DIRECT_URL = \"https://example.com/cervical_data.zip\"  # Replace with actual URL\n",
    "    \n",
    "    # Option 3: Local file\n",
    "    LOCAL_FILE = RAW_DATA_DIR / \"cervical_data.zip\"  # Place your file here\n",
    "    \n",
    "    # Choose your data source method\n",
    "    download_success = False\n",
    "    archive_path = None\n",
    "    \n",
    "    # Try Google Drive first (if ID is provided)\n",
    "    if GDRIVE_FILE_ID != \"YOUR_GOOGLE_DRIVE_FILE_ID_HERE\":\n",
    "        archive_path = RAW_DATA_DIR / \"via-cervix.zip\" # Corrected filename\n",
    "        download_success = download_from_gdrive(GDRIVE_FILE_ID, archive_path)\n",
    "    \n",
    "    # Try direct URL if Google Drive failed\n",
    "    elif DIRECT_URL != \"https://example.com/cervical_data.zip\":\n",
    "        archive_path = RAW_DATA_DIR / \"cervical_data_url.zip\"\n",
    "        download_success = download_from_url(DIRECT_URL, archive_path)\n",
    "    \n",
    "    # Try local file\n",
    "    elif LOCAL_FILE.exists():\n",
    "        archive_path = LOCAL_FILE\n",
    "        download_success = True\n",
    "        print(f\"Using local file: {archive_path}\")\n",
    "    \n",
    "    # Check for any existing archives\n",
    "    else:\n",
    "        print(\"No data source configured. Looking for existing files...\")\n",
    "        for pattern in [\"*.zip\", \"*.tar\", \"*.tar.gz\", \"*.tgz\"]:\n",
    "            existing_files = list(RAW_DATA_DIR.glob(pattern))\n",
    "            if existing_files:\n",
    "                archive_path = existing_files[0]\n",
    "                download_success = True\n",
    "                print(f\"Found existing archive: {archive_path}\")\n",
    "                break\n",
    "    \n",
    "    if not download_success or not archive_path:\n",
    "        print(\"\\n❌ No data source available. Please:\")\n",
    "        print(\"1. Set GDRIVE_FILE_ID to your Google Drive file ID, OR\")\n",
    "        print(\"2. Set DIRECT_URL to a direct download link, OR\") \n",
    "        print(\"3. Place your data file in the raw_data directory\")\n",
    "        print(\"\\nFor Google Drive:\")\n",
    "        print(\"- Share your file publicly (Anyone with link can view)\")\n",
    "        print(\"- Copy the file ID from the sharing URL\")\n",
    "        return False\n",
    "    \n",
    "    # Extract the archive\n",
    "    extract_dir = RAW_DATA_DIR / \"extracted\"\n",
    "    extract_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if extract_archive(archive_path, extract_dir):\n",
    "        print(\"Archive extracted successfully\")\n",
    "    else:\n",
    "        print(\"❌ Failed to extract archive\")\n",
    "        return False\n",
    "    \n",
    "    # Organize data for SSL\n",
    "    print(\"\\nOrganizing data for semi-supervised learning...\")\n",
    "    class_counts = organize_ssl_structure(extract_dir, PROCESSED_DATA_DIR)\n",
    "    \n",
    "    if sum(class_counts.values()) == 0:\n",
    "        print(\"❌ No images found in the extracted data\")\n",
    "        print(\"Please check the archive contents and directory structure\")\n",
    "        return False\n",
    "    \n",
    "    # Print summary\n",
    "    print_data_summary(PROCESSED_DATA_DIR, class_counts)\n",
    "    \n",
    "    # Validate structure\n",
    "    if not validate_ssl_structure(PROCESSED_DATA_DIR):\n",
    "        print(\"❌ Data validation failed\")\n",
    "        return False\n",
    "    \n",
    "    # Create manifest\n",
    "    manifest = create_data_manifest(PROCESSED_DATA_DIR)\n",
    "    \n",
    "    # Create metadata for next notebooks\n",
    "    metadata = {\n",
    "        \"data_dir\": str(PROCESSED_DATA_DIR),\n",
    "        \"labeled_dir\": str(PROCESSED_DATA_DIR / \"labeled\"),\n",
    "        \"unlabeled_dir\": str(PROCESSED_DATA_DIR / \"unlabeled\"),\n",
    "        \"classes\": [\"Negative\", \"Positive\"],\n",
    "        \"num_classes\": 2,\n",
    "        \"ssl_enabled\": class_counts.get(\"Unlabeled\", 0) > 0,\n",
    "        \"class_mapping\": {\n",
    "            \"0\": \"Negative (Normal/Healthy)\", \n",
    "            \"1\": \"Positive (Abnormal/Suspicious)\"\n",
    "        },\n",
    "        \"file_counts\": dict(class_counts),\n",
    "        \"binary_classification\": True\n",
    "    }\n",
    "    \n",
    "    metadata_path = ARTIFACTS_DIR / \"data_metadata.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nMetadata saved: {metadata_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA INGESTION COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"✅ Data organized for semi-supervised learning\")\n",
    "    print(\"✅ Binary classification setup (Negative vs Positive)\")\n",
    "    print(\"✅ Ready for model preparation\")\n",
    "    print(f\"\\nNext step: Run 02_prepare_base_model.ipynb\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f4e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CERVICAL IMAGE DATA INGESTION FOR SSL\n",
      "============================================================\n",
      "Data source options:\n",
      "1. Google Drive file ID\n",
      "2. Direct URL\n",
      "3. Local file path\n",
      "4. Skip download (data already extracted)\n",
      "Downloading data from Google Drive...\n",
      "URL: https://drive.google.com/uc?id=1Sw8aSal9R2Kh7PoD6Ma3QpXevCh2Kcrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Sw8aSal9R2Kh7PoD6Ma3QpXevCh2Kcrs\n",
      "From (redirected): https://drive.google.com/uc?id=1Sw8aSal9R2Kh7PoD6Ma3QpXevCh2Kcrs&confirm=t&uuid=7dc23493-2bc6-44c9-807e-9582aebf99ac\n",
      "To: c:\\09_AHFID\\CervicalAI-Screen\\artifacts\\raw_data\\via-cervix.zip\n",
      "100%|██████████| 138M/138M [08:31<00:00, 269kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: artifacts\\raw_data\\via-cervix.zip\n",
      "Extracting artifacts\\raw_data\\via-cervix.zip to artifacts\\raw_data\\extracted\n",
      "Extraction completed\n",
      "Archive extracted successfully\n",
      "\n",
      "Organizing data for semi-supervised learning...\n",
      "Using source directory: artifacts\\raw_data\\extracted\\via-cervix\n",
      "Copied 92 images from Negative\n",
      "Copied 98 images from Positive\n",
      "Copied 6377 images from Unlabeled\n",
      "\n",
      "============================================================\n",
      "DATA ORGANIZATION SUMMARY\n",
      "============================================================\n",
      "LABELED DATA:\n",
      "  Negative: 92 images\n",
      "  Positive: 98 images\n",
      "  Total labeled: 190 images\n",
      "\n",
      "UNLABELED DATA:\n",
      "  Unlabeled: 6377 images\n",
      "\n",
      "TOTAL DATASET: 6567 images\n",
      "Unlabeled/Labeled ratio: 33.6:1\n",
      "SSL data ratio: 33.6x more unlabeled data\n",
      "✓ Excellent ratio for semi-supervised learning\n",
      "\n",
      "BINARY CLASSIFICATION SETUP:\n",
      "  Class 0 (Negative): Normal/healthy cervical images\n",
      "  Class 1 (Positive): Abnormal/suspicious cervical images\n",
      "  This binary setup is optimal for screening applications\n",
      "\n",
      "VALIDATING SSL DATA STRUCTURE...\n",
      "✅ Data structure validation passed\n",
      "Data manifest saved: artifacts\\via_cervix_ssl\\data_manifest.json\n",
      "\n",
      "Metadata saved: artifacts\\data_metadata.json\n",
      "\n",
      "============================================================\n",
      "DATA INGESTION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "✅ Data organized for semi-supervised learning\n",
      "✅ Binary classification setup (Negative vs Positive)\n",
      "✅ Ready for model preparation\n",
      "\n",
      "Next step: Run 02_prepare_base_model.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TROUBLESHOOTING GUIDE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"If data ingestion failed, try:\")\n",
    "        print(\"1. Check your internet connection\")\n",
    "        print(\"2. Verify Google Drive file permissions\")\n",
    "        print(\"3. Ensure the archive contains cervical image folders\")\n",
    "        print(\"4. Check file formats (supported: ZIP, TAR, TAR.GZ)\")\n",
    "        print(\"5. Verify folder structure contains class directories\")\n",
    "else:\n",
    "    print(\"Script loaded. Run main() to execute data ingestion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
